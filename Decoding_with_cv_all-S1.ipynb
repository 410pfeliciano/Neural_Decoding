{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages\n",
    "\n",
    "Below, we import both standard packages, and functions from the accompanying .py files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from preprocessing_funcs import get_spikes_with_history\n",
    "\n",
    "#Import metrics\n",
    "from metrics import get_R2\n",
    "from metrics import get_rho\n",
    "\n",
    "#Import decoder functions\n",
    "from decoders import WienerCascadeDecoder\n",
    "from decoders import WienerFilterDecoder\n",
    "from decoders import DenseNNDecoder\n",
    "from decoders import SimpleRNNDecoder\n",
    "from decoders import GRUDecoder\n",
    "from decoders import LSTMDecoder\n",
    "from decoders import XGBoostDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "The data that we load is in the format described below. We have another example script, \"neural_preprocessing.py\" that may be helpful towards putting the data in this format.\n",
    "\n",
    "Neural data should be a matrix of size \"number of time bins\" x \"number of neurons\", where each entry is the firing rate of a given neuron in a given time bin\n",
    "\n",
    "The output you are decoding should be a matrix of size \"number of time bins\" x \"number of features you are decoding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# folder='/Users/jig289/Dropbox/MATLAB/Projects/In_Progress/BMI/Processed_Data/'\n",
    "# folder='/Users/jig289/Dropbox/Grad_School/Research/Projects/In_Progress/Decoding/DataFiles/'\n",
    "folder='/home/jglaser2/Data/DecData/'\n",
    "\n",
    "with open(folder+'s1_test_data.pickle','rb') as f:\n",
    "    neural_data,vels_binned,pos_binned,acc_binned=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. User Inputs\n",
    "The user can define what time period to use spikes from (with respect to the output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins_before=13 #How many bins of neural data prior to the output are used for decoding\n",
    "bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "bins_after=0 #How many bins of neural data after (and including) the output are used for decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Format Covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Input Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "# Function to get the covariate matrix that includes spike history from previous bins\n",
    "X=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "\n",
    "# Format for Wiener Filter, Wiener Cascade, XGBoost, and Dense Neural Network\n",
    "#Put in \"flat\" format, so each \"neuron / time\" is a single feature\n",
    "X_flat=X.reshape(X.shape[0],(X.shape[1]*X.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format Output Covariates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set decoding output\n",
    "y=vels_binned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C. Process Covariates\n",
    "We normalize (z_score) the inputs and zero-center the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Z-scoring function that works with Nans:\n",
    "def zscore_nan(X,axis):\n",
    "    X_zscore=(X - np.nanmean(X,axis=axis)) / np.nanstd(X,axis=axis)\n",
    "    return X_zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalize inputs\n",
    "X=zscore_nan(X,axis=0)\n",
    "X_flat=zscore_nan(X_flat,axis=0)\n",
    "\n",
    "#Zero-center outputs\n",
    "y_mean=np.mean(y,axis=0)\n",
    "y=y-y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D. Split into training/testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set what part of data should be part of the training/testing/validation sets\n",
    "training_range=[0, 0.5]\n",
    "testing_range=[0.5, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_examples=X.shape[0]\n",
    "\n",
    "#Note that each range has a buffer of\"bins_before\" bins at the beginning, and \"bins_after\" bins at the end\n",
    "#This makes it so that the different sets don't include overlapping neural data\n",
    "training_set=np.arange(np.int(np.round(training_range[0]*num_examples))+bins_before,np.int(np.round(training_range[1]*num_examples))-bins_after)\n",
    "testing_set=np.arange(np.int(np.round(testing_range[0]*num_examples))+bins_before,np.int(np.round(testing_range[1]*num_examples))-bins_after)\n",
    "\n",
    "#Get training data\n",
    "X_train=X[training_set,:,:]\n",
    "X_flat_train=X_flat[training_set,:]\n",
    "y_train=y[training_set,:]\n",
    "\n",
    "#Get testing data\n",
    "X_test=X[testing_set,:,:]\n",
    "X_flat_test=X_flat[testing_set,:]\n",
    "y_test=y[testing_set,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4A. Wiener Filter (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_wf=WienerFilterDecoder()\n",
    "#Fit model\n",
    "model_wf.fit(X_flat_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4B. Wiener Cascade (Linear Nonlinear Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_wc=WienerCascadeDecoder(degree=5)\n",
    "#Fit model\n",
    "model_wc.fit(X_flat_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4C. XGBoost (Extreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_xgb=XGBoostDecoder(max_depth=2,num_round=1000)\n",
    "#Fit model\n",
    "model_xgb.fit(X_flat_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4D. Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_dnn=DenseNNDecoder(units=[400,400],dropout=0,num_epochs=15)\n",
    "#Fit model\n",
    "model_dnn.fit(X_flat_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4E. Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_rnn=SimpleRNNDecoder(units=400,dropout=0,num_epochs=10)\n",
    "#Fit model\n",
    "model_rnn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4F. GRU (Gated Recurrent Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_gru=GRUDecoder(units=400,dropout=.25,num_epochs=5)\n",
    "#Fit model\n",
    "model_gru.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4G. LSTM (Long Short Term Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Declare model\n",
    "model_lstm=LSTMDecoder(units=400,dropout=0,num_epochs=15)\n",
    "#Fit model\n",
    "model_lstm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Model Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('s1_models.pickle','wb') as f:\n",
    "#     pickle.dump([model_wf,model_wc,model_xgb,model_dnn,model_rnn,model_gru,model_lstm],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get Fits on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_tests=10\n",
    "num_test_examples=y_test.shape[0]\n",
    "num_examples_per_test=np.round(np.divide(num_test_examples,num_tests))\n",
    "\n",
    "#Initialize\n",
    "mean_R2_wf=np.empty(num_tests)\n",
    "mean_R2_wc=np.empty(num_tests)\n",
    "mean_R2_xgb=np.empty(num_tests)\n",
    "mean_R2_dnn=np.empty(num_tests)\n",
    "mean_R2_rnn=np.empty(num_tests)\n",
    "mean_R2_gru=np.empty(num_tests)\n",
    "mean_R2_lstm=np.empty(num_tests)\n",
    "\n",
    "for i in range(num_tests):\n",
    "\n",
    "    idx=np.arange(num_examples_per_test*i,num_examples_per_test*(i+1))\n",
    "    X_test_temp=X_test[idx,:,:]\n",
    "    X_flat_test_temp=X_flat_test[idx,:]\n",
    "    y_test_temp=y_test[idx,:]\n",
    "\n",
    "    #Wiener Filter\n",
    "    y_predicted_wf=model_wf.predict(X_flat_test_temp)\n",
    "    mean_R2_wf[i]=np.mean(get_R2(y_test_temp,y_predicted_wf))\n",
    "    \n",
    "    #Wiener Cascade\n",
    "    y_predicted_wc=model_wc.predict(X_flat_test_temp)\n",
    "    mean_R2_wc[i]=np.mean(get_R2(y_test_temp,y_predicted_wc))   \n",
    "    \n",
    "    #XGBoost\n",
    "    y_predicted_xgb=model_xgb.predict(X_flat_test_temp)\n",
    "    mean_R2_xgb[i]=np.mean(get_R2(y_test_temp,y_predicted_xgb))        \n",
    "    \n",
    "    #DNN\n",
    "    y_predicted_dnn=model_dnn.predict(X_flat_test_temp)\n",
    "    mean_R2_dnn[i]=np.mean(get_R2(y_test_temp,y_predicted_dnn))   \n",
    "    \n",
    "    #RNN\n",
    "    y_predicted_rnn=model_rnn.predict(X_test_temp)\n",
    "    mean_R2_rnn[i]=np.mean(get_R2(y_test_temp,y_predicted_rnn))\n",
    "    \n",
    "    #GRU\n",
    "    y_predicted_gru=model_gru.predict(X_test_temp)\n",
    "    mean_R2_gru[i]=np.mean(get_R2(y_test_temp,y_predicted_gru))\n",
    "    \n",
    "    #LSTM\n",
    "    y_predicted_lstm=model_lstm.predict(X_test_temp)\n",
    "    mean_R2_lstm[i]=np.mean(get_R2(y_test_temp,y_predicted_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.67550371087286465, 0.0086567180384587699)\n",
      "(0.69255356886578234, 0.0071501963754975074)\n",
      "(0.71860100341136934, 0.0071469434400783394)\n",
      "(0.66458018275342634, 0.01170739267412285)\n",
      "(0.74256033941068267, 0.0057448139449299759)\n",
      "(0.78488981144494852, 0.0058230462032647099)\n",
      "(0.7871825682066883, 0.005618281590838793)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(mean_R2_wf),np.std(mean_R2_wf)/np.sqrt(10))\n",
    "print(np.mean(mean_R2_wc),np.std(mean_R2_wc)/np.sqrt(10))\n",
    "print(np.mean(mean_R2_xgb),np.std(mean_R2_xgb)/np.sqrt(10))\n",
    "print(np.mean(mean_R2_dnn),np.std(mean_R2_dnn)/np.sqrt(10))\n",
    "print(np.mean(mean_R2_rnn),np.std(mean_R2_rnn)/np.sqrt(10))\n",
    "print(np.mean(mean_R2_gru),np.std(mean_R2_gru)/np.sqrt(10))\n",
    "print(np.mean(mean_R2_lstm),np.std(mean_R2_lstm)/np.sqrt(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_folder='/home/jglaser2/Data/Decoding_Results/'\n",
    "with open(save_folder+'s1_results.pickle','wb') as f:\n",
    "    pickle.dump([mean_R2_wf,mean_R2_wc,mean_R2_xgb,mean_R2_dnn,mean_R2_rnn,mean_R2_gru,mean_R2_lstm],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
